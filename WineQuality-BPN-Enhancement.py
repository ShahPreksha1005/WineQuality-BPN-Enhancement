# -*- coding: utf-8 -*-
"""WineQuality-BPN-Enhancement.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zyk2AatN5we9H7t20kOm3FJ8Iul-cRTC

# **Implementing Learning Factors in Backpropagation Neural Networks (BPN)**

## **Created by: Preksha Shah**

## **Domain: Wine Quality Assessment**

## **Problem Statement**

The task is to implement a Backpropagation Neural Network (BPN) with learning factors to enhance the training process and improve convergence speed.


## **1. Loading and Preparing the Dataset**

### **1.1 Import Necessary Libraries**
"""

# Install and import necessary libraries
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt

"""### **1.2 Load the Dataset**"""

# Function to clean and preprocess the CSV file
def clean_csv(file_path):
    with open(file_path, 'r') as file:
        lines = file.readlines()

    # Check for and fix any issues in the lines
    clean_lines = []
    for line in lines:
        # Remove any irregular quotation marks or delimiters
        clean_line = line.replace(';"', ';').replace('";', ';').replace('"', '')
        clean_lines.append(clean_line)

    # Write the cleaned lines to a new CSV file
    clean_file_path = '/content/cleaned_winequality-white.csv'
    with open(clean_file_path, 'w') as file:
        file.writelines(clean_lines)

    return clean_file_path

# Clean the CSV file
clean_file_path = clean_csv('/content/winequality-white.csv')

# Read the cleaned dataset into a pandas DataFrame
data = pd.read_csv(clean_file_path, delimiter=';', engine='python')

"""## **2. Basic Data Exploration**

### **2.1 Display First Few Rows and Basic Info**

"""

# Display the first few rows of the dataset
print(data.head())

# Display basic information about the dataset
print("\nDataset Info:")
print(data.info())

# Display summary statistics of the dataset
print("\nSummary Statistics:")
print(data.describe())

"""####**Key Insights:**

- **Fixed Acidity**: Shows a relatively narrow distribution with a mean around 6.85.
- **Volatile Acidity**: Majority of values lie between 0.21 to 0.32, indicating moderate volatility.
- **Citric Acid**: Generally distributed around 0.33, with some instances reaching up to 1.66.
- **Residual Sugar**: Wide range from 0.6 to 65.8, suggesting varied sweetness levels.
- **Chlorides**: Mostly concentrated between 0.036 to 0.05, reflecting typical salt content.
- **Free Sulfur Dioxide**: Concentrated between 23 to 46, with a maximum of 289, indicating variability.
- **Total Sulfur Dioxide**: Generally distributed around 138, but with a wide range up to 440.
- **Density**: Mean density close to 0.994, indicative of liquid density.
- **pH**: Centrally distributed around 3.18, ranging from 2.72 to 3.82.
- **Sulphates**: Mean around 0.49, reflecting typical sulfates content.
- **Alcohol**: Mean alcohol content approximately 10.5%, with values ranging from 8% to 14.2%.
- **Quality**: Majority rated between 5 and 6, with extremes at 3 and 9, showing diversity in quality ratings.

### **2.2 Univariate Analysis**
"""

# Visualize distributions of relevant variables
plt.figure(figsize=(12, 6))

plt.subplot(2, 2, 1)
plt.hist(data['fixed acidity'], bins=20, color='blue', edgecolor='black')
plt.title('Distribution of Fixed Acidity')
plt.xlabel('Fixed Acidity')
plt.ylabel('Frequency')

plt.subplot(2, 2, 2)
plt.hist(data['volatile acidity'], bins=20, color='green', edgecolor='black')
plt.title('Distribution of Volatile Acidity')
plt.xlabel('Volatile Acidity')
plt.ylabel('Frequency')

plt.subplot(2, 2, 3)
plt.hist(data['citric acid'], bins=20, color='orange', edgecolor='black')
plt.title('Distribution of Citric Acid')
plt.xlabel('Citric Acid')
plt.ylabel('Frequency')

plt.subplot(2, 2, 4)
plt.hist(data['pH'], bins=20, color='red', edgecolor='black')
plt.title('Distribution of pH')
plt.xlabel('pH')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

"""###**Insights:**

- **Fixed Acidity**: Indicates the primary acidity level in the wine, which typically affects its taste and structure. Wines with higher fixed acidity may taste more tart.
  
- **Volatile Acidity**: Reflects the presence of acetic acid in wine, which can contribute to vinegar-like flavors if too high. The distribution suggests most wines have moderate volatile acidity.

- **Citric Acid**: Adds freshness and flavor to wines, especially white wines. The distribution shows varying levels of citric acid, influencing wine's acidity profile and taste.

- **pH**: Measures acidity on a scale where lower pH values indicate higher acidity. Wines with pH values closer to 3.0 are likely more acidic, affecting taste and stability.

#### **Insights:**



### **2.3 Bivariate Analysis**
"""

# Create correlation heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm', annot_kws={'size': 10}, fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()

"""#### **Insights:**

- **Fixed Acidity**: Shows a positive correlation with citric acid and density, suggesting wines with higher fixed acidity might also exhibit higher levels of citric acid and density.
  
- **Volatile Acidity**: Displays negative correlations with citric acid and quality, implying that wines with higher volatile acidity may have lower levels of citric acid and potentially lower quality ratings.
  
- **Citric Acid**: Positively correlates with fixed acidity and negatively with volatile acidity, indicating that wines with higher citric acid content might also have higher fixed acidity and lower volatile acidity.

- **pH**: Strong negative correlation with fixed acidity suggests that wines with higher fixed acidity tend to have lower pH levels.

- **Alcohol**: Shows positive correlations with quality, indicating that wines with higher alcohol content may receive higher quality ratings.

## **3. Data Preprocessing**
### **3.1 Split the Dataset**


"""

# Split the data into features (X) and target (y)
X = data.drop('quality', axis=1)
y = data['quality']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""### **3.2 Standardize or Normalize the Data**"""

# Standardize the features using StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""## **4. Implementing Backpropagation Neural Network (BPN) with Learning Factors**

### **4.1 Define Neural Network Architecture**
"""

# Define your neural network architecture using TensorFlow/Keras
model = Sequential([
    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.4),  # Increased dropout rate for better regularization
    Dense(128, activation='relu'),
    Dropout(0.4),
    Dense(64, activation='relu'),
    Dropout(0.4),
    Dense(32, activation='relu'),
    Dense(1, activation='linear')
])

"""### **4.2 Implement Backpropagation with Learning Factors**"""

# Implement the backpropagation algorithm with learning factors
optimizer = Adam(learning_rate=0.0001)
model.compile(optimizer=optimizer, loss='mean_squared_error')

"""## **5. Training and Evaluating the Model**

### **5.1 Compile the Model**
"""

# Define callbacks for early stopping and reducing learning rate on plateau
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=8, min_lr=0.00001)

"""### **5.2 Train the Model**"""

# Train your model, specifying batch size, epochs, and validation data
history = model.fit(X_train, y_train, epochs=150, batch_size=32,
                    validation_data=(X_test, y_test), verbose=1,
                    callbacks=[early_stopping, reduce_lr])

"""###**Insights:**

- **Loss Reduction**: The model's training loss decreases steadily from 11.8636 to 0.5772 over 59 epochs, indicating effective learning.

- **Validation**: Validation loss decreases similarly, showing the model generalizes well to unseen data.

- **Learning Rate**: Starts at 5.0000e-04 and decreases to 1.0000e-04 after epoch 26, helping optimize model performance.

### **5.3 Evaluate the Model**
"""

# Evaluate your model on test data and print performance metrics
test_loss = model.evaluate(X_test, y_test)
print(f'Test Loss: {test_loss}')

"""#### **Insights:**

A test loss of 1.0628 indicates that the model is performing adequately on unseen data, but further analysis and comparison with other models or benchmarks would provide a more comprehensive evaluation.

## **6. Visualizing the Training Process**

"""

# Plot training and validation loss curves over epochs
plt.figure(figsize=(12, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""**Insights from Visualization:**

The plot of training and validation loss over epochs gives a snapshot of how the model learns from the data. It shows:

- **Training Loss**: Decreases steadily as the model improves its predictions on the training data.
- **Validation Loss**: Ideally decreases and stabilizes, reflecting how well the model generalizes to new data.
- **Overfitting Check**: Look for a divergence where training loss continues to decrease while validation loss flattens or increases, indicating potential overfitting.
  
Overall, lower final loss values indicate better model performance, balancing between learning from training data and generalizing to new data effectively.

---

## **7. Key Achievements**

- Implemented a Backpropagation Neural Network (BPN) with learning factors to improve training efficiency.
- Conducted thorough data preprocessing, including cleaning and standardization.
- Designed and trained a TensorFlow/Keras neural network for wine quality prediction.
- Applied effective training strategies like early stopping and learning rate reduction.
- Analyzed training and validation loss curves to monitor model performance.

## **8. Conclusion**

This assignment successfully implemented a Backpropagation Neural Network with learning factors for wine quality assessment. Through meticulous data preprocessing and model training, we achieved effective predictions and gained insights into wine quality factors. Moving forward, optimizing hyperparameters and exploring advanced architectures could further enhance model performance in similar applications.

---
---
"""